[
["index.html", "Statistische Modellbildung Einleitung", " Statistische Modellbildung Walter Gruber 2019-01-06 Einleitung Allgemein versteht man unter Modellbau die Herstellung eines konkreten, dreidimensionalen, physischen Objektes. Der Prozess der Modellbildung abstrahiert dabei mit dem Erstellen eines Modells von der Realität, weil diese meist zu komplex ist, um sie vollständig abzubilden. Diese Vollständigkeit wird aber auch gar nicht beabsichtigt, vielmehr sollen lediglich die wesentlichen Einflussfaktoren identifiziert und dargestellt werden, die für den realen Prozess und im Modellkontext bedeutsam sind. Der Begriff Modell entstand im Italien der Renaissance (italienisch modello), hervorgegangen aus lateinischesn modulus, einem Maßstab in der Architektur. Es wurde bis ins 18. Jahrhundert vorwiegend in der bildenden Kunst als Fachbegriff verwendet. In der Umgangssprache wird das Wort Modell für unterschiedliche Sachverhalte verwendet. Im naturwissenschaftlichen Erkenntnisprozess haben Modelle verschiedene Bedeutungen und Funktionen. Merkmale und Charakteristika, an denen ein Modell eindeutig als Modell definiert werden kann, existieren nicht (Upmeier 2010). Die Bedeutung ist zudem vom Kontext abhängig. Beim Modellsein werden folgende Aspekte unterschieden (Mahr 2008): Modell für etwas sein Modell von etwas sein Dies muss auch bei der Beurteilung eines Modells, also der Frage, ob ein Modell ein gutes Modell ist, berücksichtigen werden. Hier ist die Frage des Zwecks des Modells entscheidend. Mahr benennt drei allgemeine Kriterien zur Beurteilung von Modellen: Das Modell muss die Funktion erfüllen, durch seine Anwendung etwas von dem, wovon es ein Modell ist, zudem wofür es ein Modell ist, transportieren. Das Modell muss ein Garant von Konsistenz sein. D.h., dass das Modell garantieren muss, dass es keine Widersprüche enthält, so dass seine Anwendung nicht notwendig zu Widersprüchen führen muss. Das Modell muss über eine ausreichende pragmatische Eignung verfügen und demnach das, wofür oder wovon es ein Modell ist, ausreichend und angemessen repräsentieren. Die Hauptfunktion in den Naturwissenschaften ist die Untersuchung und Interpretation von Phänomenen. Ziel ist die Reduzierung von Komplexität und somit die Erzeugung eines fokussierten Bildes des zu untersuchenden Objekts. Man kann sagen, dass Modelle den Blick auf das Wesentliche eines Phänomens oder Gegenstandes lenken sollen. Das Modell ist somit ein Repräsentant des Originals. Modell und Original können sich aber im Material, der Dimensionierung und der Abstraktion unterscheiden. Sie können gegenständlich, bildhaft, schematisch oder formelhaft sein. References "],
["vorbereitung-fur-r-studio.html", "Vorbereitung für R-Studio", " Vorbereitung für R-Studio Für die Bearbeitung der Aufgaben und Beispiele in diesem Kurs, sollte der folgende Code am Anfang jeder neuen R-Datei übernommen werden. Die Vorgehensweise ist: Starten von R-Studio Öffnen einer neuen R-Script Datei Kopiere die nachfolgenden Zeilen in diese Datei Speichere die Datei mit einem entsprechenden Namen Führe diesen Code aus Füge deinen Code nach diesen Zeilen ein # Initialisierung rm(list = ls()) if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load(corrplot, DAAG, dataMaid, devtools, doBy, DT, ggformula, ggplot2, gridExtra, htmlwidgets, imager, knitr, labelled, leaps, magick, MASS, NHANES, mosaic, mosaicCore, mosaicData, pander, pastecs, ppcor, reshape2, rockchalk, rpart, rpart.plot) "],
["statistisches-modell.html", "Statistisches Modell Modellbildung Kennzeichen eines statistischen Modelles Grundlagen der statistischen Modellbildung", " Statistisches Modell Unter statistischer Modellbildung versteht man den Prozess, ein passendes Modell für die Daten einer Beobachtungsreihe zu finden. Als Prozess wird die strukturierte und gesteuerte Reihe von Arbeitsschritten bezeichnet, konzipiert, um eine bestimmtes Ergebnis hervorzubringen. Daten sind Werte und Beobachtungen, die im Lauf einer (statistischen) Erhebung gesammelt werden. Eine Erhebung ist die Sammlung von Daten einer bestimmten Grundgesamtheit zum Zweck der Untersuchung eines speziellen Aspektes. Die Daten werden oft nur von einer Stichprobe der Grundgesamtheit erhoben. Erhebungen sind in der Forschung weit verbreitet. Eine Stichprobe ist die Teilmenge einer Grundgesamtheit bei einer statistischen Untersuchung, zusammengestellt, um ausgewählte Eigenschaften der Gesamtpopulation zu untersuchen. Modellbildung Die Modellbildung abstrahiert mit dem Erstellen eines Modells von der Realität, weil diese meist zu komplex ist, um sie vollständig abzubilden. Man unterscheidet dabei: Strukturelle Modellbildung: Bei struktureller Modellbildung ist die innere Struktur des Systems bekannt, es wird jedoch bewusst abstrahiert, modifiziert und reduziert. Man spricht hier von einem Whitebox-Modell. Pragmatische Modellbildung: Bei pragmatischer Modellbildung ist die innere Struktur des Systems unbekannt, es lässt sich nur das Verhalten bzw. die Interaktion des Systems beobachten und modellieren. Die Hintergründe lassen sich meist nicht oder nur zum Teil verstehen - hier spricht man von einem Blackbox-Modell. Mischformen: Bei Mischformen sind Teile des Systems bekannt, andere wiederum nicht. Nicht alle Wechselwirkungen und Interaktionen zwischen Teilkomponenten lassen sich nachvollziehen - hier spricht man vom Greybox-Modell. Diese Mischform ist die häufigste, weil es aufgrund von Kosten-Nutzen-Überlegungen meist ausreichend ist, das System auf diese Weise abzubilden. Kennzeichen eines statistischen Modelles Ein Modell ist im Wesentlichen durch drei Merkmale gekennzeichnet: Abbildung: eines natürlichen oder eines künstlichen Originals, wobei dieses Original selbst auch wiederum ein Modell sein kann. Verkürzung: es erfasst im Allgemeinen nicht alle Eigenschaften (Attribute) des Originals, sondern nur diejenigen, die dem Modellschaffer bzw. Modellnutzer relevant erscheinen. Diese werden häufig in Form von aggregierenden Maßzahlen (Parameter) beschrieben. Pragmatismus: sind ihren Originalen nicht eindeutig zugeordnet. Sie erfüllen ihre Ersetzungsfunktion: für bestimmte Subjekte (für wen?) innerhalb bestimmter Zeitintervalle (wann?) unter Einschränkung auf bestimmte gedankliche oder tatsächliche Operationen (wozu?). Statistische Modelle sind somit: eine vereinfachte mathematisch-formalisierte Methode, sich der Realität anzunähern. Beschreiben den Zustand eines Systems vor und nach Änderungen äußerer Relationen, nicht jedoch während einer Änderung. Das Ziel ist es herauszufinden, ob man in der Natur auftretende Phänomene auf allgemein gültige Gesetzmäßigkeiten zurückführen kann. In der Regel werden Beobachtungen durchgeführt und diese als Daten aufgezeichnet. In diesen Daten gilt es Muster zu finden, die Rückschlüsse auf die Mechanismen zulassen, welche dem Phänomen zugrunde liegen. Auf diese Weise wird ein Modell von der Funktionsweise eines Phänomens erstellt. Statistische Modelle finden Verwendung für: Annäherung (Approximation) Erklärung Vorhersage Zyklus der Modellbildung Das Bilden von statistischen Modellen ist ein iterativer Vorgang, welcher durchaus mehrere zyklische Entwicklungsschritte beinhalten kann. Abbildung 1: Zyklus der Modellbildung Grundlagen der statistischen Modellbildung Grundlage ist eine entsprechende Fragestellung, die auf theoretischen Grundlagen basiert und möglichst präzise formuliert werden soll. Bei den Daten, die zur Beantwortung der Fragestellung geeignet sind, sollte man speziell achten auf: woher Sie kommen. was und wie Sie ein erhobenes/gemessenes Objekt abbilden. Charakteristiken von guten Fragen sind: Interessant für andere: wie z.B. für Mitarbeiter, wissenschaftliche Gemeinschaft, Geldgeber, Allgemeinheit? Noch nicht beantwortet: wurde die Fragestellung bereits bearbeitet/beantwortet? Literaturrecherche, Kongressbeiträge, etc. Sinnvoll: kann durch die Beantwortung eine Erklärung gefunden werden, wie etwas funktioniert? Beantwortbarkeit: kann die Fragestellung überhaupt beantwortet werden? Spezifisch: wie präzise ist die Fragestellung? Gesundes Essen führt zu besserer Gesundheit ist weniger präzise wie z.B. 5 Mal am Tag Früchte und Gemüse führt zu einer geringeren Wahrscheinlichkeit an Atemwegserkrankungen zu erkranken. Bei den Qualitätskriterien für Daten ist folgendes zu beachten: Korrektheit: müssen mit der Realität übereinstimmen. Konsistenz: dürfen in sich und zu anderen Datensätzen keine Widersprüche aufweisen. Zuverlässigkeit: Entstehung der Daten muss nachvollziehbar sein. Vollständigkeit: muss alle notwendigen Attribute enthalten. Genauigkeit: müssen in der jeweils geforderten Exaktheit vorliegen (Beispiel: Nachkommastellen). Aktualität: müssen jeweils dem aktuellen Zustand der abgebildeten Realität entsprechen. Relevanz: Der Informationsgehalt muss den jeweiligen Informationsbedarf erfüllen. Einheitlichkeit: Die Informationen müssen einheitlich strukturiert sein. Eindeutigkeit: muss eindeutig interpretierbar sein. Verständlichkeit: müssen in ihrer Begrifflichkeit und Struktur mit den Vorstellungen der Fachbereiche übereinstimmen. Redundanzfreiheit: Innerhalb der Datensätze sollen/dürfen keine Dubletten vorkommen. Aus der Testtheorie sind in auch die Gütekriterien (Testgütekriterien) der empirischen Forschung für die statistische Modellbildung anzuwenden. Diese sind: Reliabilität: Indikator für die Replizierbarkeit der Ergebnisse. Fragen müssen z.B. so eindeutig formuliert sein, dass sie nicht höchst unterschiedlich verstanden werden können. Validität: wenn die gewählten Indikatoren, Fragen und Antwortmöglichkeiten wirklich und präzise das messen, was gemessen werden soll. Objektivität: wenn die Wahl der Messenden, InterviewerInnen, PrüferInnen keinen Einfluss auf die Ergebnisse hat. Bei der Analyse und Interpretation der Daten unterscheidet man: Deskriptiv: beschreibend (innerhalb Daten) Explorativ: Hypothesen generierend (innerhalb Daten) Inferentiell: Statement über etwas, was nicht beobachtet wird, also aus den erhobenen Daten auf die Ursachen zu schliessen, die die Daten erzeugt haben könnten (außerhalb der Daten). Prädiktiv: Vorhersage für Werte die noch nicht beobachtet wurden (außerhalb der Daten) Korrelativ: Beschreibung der Zusammenhäng zwischen Beobachtungen (innerhalb und außerhalb der Daten). Mechanistisch: nicht nur die Frage ob ein Zusammenhang besteht, sondern wie und warum der Zusammenhang besteht ist von Interesse. Deskriptive Statistik Methoden der deskriptiven Statistik umfassen: Tabellen Diagramme Parameter (Maßzahlen, Kennzahlen, Kennwerte) Lagemaßen: Maße der zentralen Tendenz, wie z.B. Mittelwerte, Median, Modus. Streuungsmaße: für die Variabilität (Dispersion), wie z.B. Range, Varianz, Standardabweichung, Standardfehler. Zusammenhangsmaße: wie z.B. die Korrelation. Die deskriptive Statistik ist ein zentrales Element jeder formalen Analyse von Daten, hat jedoch in ihrer Eigenschaft folgende Einschränkungen: liefert keine Aussagen zu einer über die untersuchten Fälle hinausgehenden Grundgesamtheit . Es ist keine Überprüfung von Hypothesen möglich. Verwendet keine stochastischen Modelle (Grundlage der induktiven Statistik). getroffenen Aussagen können nicht durch Fehlerwahrscheinlichkeiten abgesichert werden. Explorative Datenanalyse (EDA) Die Methoden der explorativen Statistik sind meist identisch mit denen der deskriptiven Statistik, unterscheiden sich aber bezüglich der Ziele der Analyse. Dieses sind in der explorativen Analyse: bisher unbekannte Strukturen und Zusammenhänge in den Daten zu finden. Annahmen (Hypothesen) über die Ursache und den Grund der beobachteten Daten zu bilden. Annahmen einzuschätzen, worauf statistische Inferenz basieren kann. Die Auswahl von passenden statistischen Werkzeugen und Techniken zu unterstützen. Eine Basis für die weitere Daten-Sammlung durch Umfragen oder Design von Experimenten bereitzustellen. Die Vorgehensweise bei einer EDA kann anhand der folgenden Checkliste kurz besch Inferenzstatistik Bei der inferentiellen Statistik wird eine Zufallsstichprobe aus einer Grundgesamtheit entnommen, um Rückschlüsse auf die Grundgesamtheit zu ziehen und diese zu beschreiben. Weitere Begriffe sind: analytische Statistik inferenzielle Statistik induktive Statistik schließende Statistik Die inferentielle Statistik ist in den Situationen nützlich, in denen es nur schwer oder gar nicht möglich ist, eine vollständige Grundgesamtheit zu untersuchen. Wesentliche Merkmale der Inferenzstatistik sind: trifft Wahrscheinlichkeitsaussagen über Populationswerte. Daten werden in Form von (Zufalls-) Stichproben aus der Grundgesamtheit entnommen. Signifikanztests und Intervalleschätzungen bieten die Entscheidungsgrundlage hinsichtlich der postulierten Hypothesen. Prädiktive Statistik Prädikative Statistik (predictive analytics) verwendet (historische) Daten, um zukünftige Ereignisse vorherzusagen. Im Allgemeinen werden vorliegende (historische) Daten verwendet, um ein mathematisches (prädiktives Modell) Modell zu erstellen. Dieses Modell soll bestmöglich wichtige Trends in den Daten erfassen. Dieses wird dann auf aktuelle Daten angewendet, um zukünftige Ereignisse vorherzusagen, oder um Aktionen vorzuschlagen, mit denen optimale Ergebnisse erreicht werden können. Vor allem in den letzten Jahren hat diese Form der Statistik in den Bereichen von Big Data und Machine Learning sehr an Bedeutung gewonnen. Mechanististische Statistik Zusammenhängen zwischen mehreren Variablen mit gleichzeitiger Interpretation der Kausalitäten werden häufig mit Hilfe eines Strukturgleichungsmodells (SEM1) analysiert. Dabei handelt es sich um ein statistisches Modell, das das Schätzen und Testen korrelativer Zusammenhänge zwischen abhängigen Variablen und unabhängigen Variablen sowie den verborgenen Strukturen dazwischen erlaubt. Eine Besonderheit von Strukturgleichungsmodellen ist das Überprüfen latenter (nicht direkt beobachtbarer) Variablen. Spezialfälle von Strukturgleichungsmodellen sind: Pfadanalyse Faktorenanalyse Regressionsanalyse Ein Strukturgleichungsmodell stellt wiederum einen Spezialfall eines sogenannten Kausalmodells dar. Structural Equation Modeling↩ "],
["datenbeschreibung.html", "Datenbeschreibung", " Datenbeschreibung Betrachten wir zunächst einen einfachen Datensatz aus dem Projekt MOSAIC Data Sets (Paket mosaicData)2. Diese Datei beinhaltet N = 534 Beobachtungen und k = 11 Variablen, deren Namen in folgender Tabelle nochmals separat angeführt sind: LNr Variablenname 1 wage 2 educ 3 race 4 sex 5 hispanic 6 south 7 married 8 exper 9 union 10 age 11 sector Angenommen Sie müssten auf Basis der vorliegenden Daten für eine Person das durchschnittliche Einkommen (wage) schätzen, ohne dabei andere Variablen zu berücksichtigen. Welchen Wert würden Sie wählen? Project MOSAIC, is a community of educators working to develop a new way to introduce mathematics, statistics, computation and modeling to students in colleges and universities.↩ "],
["deskriptive-statistik-1.html", "Deskriptive Statistik Tabellen Aufgabenstellung 1 Codebooks in R", " Deskriptive Statistik Bevor man diese Frage beantwortet, sollte man sich die deskriptive Statistik der entsprechenden Variablen genauer ansehen. In der vorliegenden Fragestellung handelt es sich um eine intervallskalierte Variable, daher ist die Betrachtung der Kennwerte für zentrale Tendenzen (Mittelwert, Median, Modus, Minimum, Maximum und Range), der Dispersion (Varianz, Standardabweichung, Quartile, Standardfehler, Konfidenzintervalle, Schiefe und Kurtosis), sowie die Darstellung der Verteilung in einem Histogramm sehr hilfreich. Tabellen Kopiere den nachfolgenden Code in den Editor und führe in aus. Diskutiere die Ergebnisse. Income &lt;- CPS85$wage library(pastecs) kable(stat.desc(Income)) # DT::datatable(data.frame(stat.desc(Income))) library(psych) kable(describe(Income)) # DT::datatable(data.frame(describe(Income))) # HÃ¤ufigkeitstabellen SR &lt;- table(CPS85$sex, CPS85$race) kable(SR) SRM &lt;- table(CPS85$sex, CPS85$race, CPS85$married) kable(SRM) # HÃ¤ufigkeitstabellen mit Randsummen x0 &lt;- addmargins(table(CPS85$sex, CPS85$race)) kable(x0) Um die Eigenschaften des Mittelwertes bei vorliegen von starken Abweichungen in den Daten noch besser zu verdeutlichen, setzen wir bei 50 zufällig gewählten Personen das Einkommen drastisch hinauf. Kopiere den folgenden Code ins RStudio und führe diesen dann aus. IncomeNew &lt;- CPS85$wage ID &lt;- sample(1:534, 50) IncomeNew[ID] &lt;- IncomeNew[ID] + 18 Aufgabenstellung 1 Erstelle nun für die neuen Daten eine Tabelle und ein Histogramm. Berechne den Mittelwert: ohne Berücksichtigung der Ausreißer (Funktion mean(???, na.rm = TRUE)) unter Berücksichtigung der Ausreißer (Funktion mean(???, trim = ???, na.rm = TRUE)) Diskutiere die Ergebnisse. Die Lösung zu diesen Aufgaben findest du in Lösung Aufgabe 1 Codebooks in R In R hat man die Möglichkeit, mit Hilfe des Pakets codebook eine genaue Beschreibung der Daten (inklusive einer deskriptiven Statistik für jede Variable) zu erstellen. Für den vorliegenden Datensatz wurde auszugsweise eines erstellt, welches in Kapitel Codebook CPS85 zu finden ist. "],
["mittelwerts-modell.html", "Mittelwerts-Modell Güteschätzung des Mittelwertsmodells Konfidenzintervall um den Mittelwert", " Mittelwerts-Modell Eine gute Schätzung wäre daher unter bestimmten Voraussetzungen (Verteilungseigenschaften) der Mittelwert, da dieser die folgende Eigenschaft besitzt: Die Summe der quadrierten Abweichungen der Beobachtungswerte \\(x_i\\) von einem beliebigen Punkt \\(m\\) wird minimal, wenn dieser Punkt \\(m = \\bar{x}\\), also der arithmetische Mittelwert ist! Damit diese Schätzung inhaltlich auch Sinn ergibt, ist jedoch eine (halbwegs) symmetrische Verteilung in Form einer Glockenkurve erforderlich. Weicht diese Verteilung stark davon ab, so wird der Mittelwert nur ein schlechtes Modell für die Population sein. Zur Veranschaulichung von Verteilungseigenschaften einer Variablen eignen sich vor allem Histogramme, Boxplots und Q-Q-Plots. Kopiere den folgenden Code ins RStudio und führe diesen dann aus. Diskutiere die Ergebnisse. # Histogramme und Density-Plots p1 &lt;- ggplot(CPS85, aes(x = wage)) + geom_histogram() p2 &lt;- ggplot(CPS85, aes(x = age)) + geom_histogram(binwidth = 4) p3 &lt;- ggplot(CPS85, aes(x = exper)) + geom_density() # Boxplots x1 &lt;- mosaic::mean_(wage ~ sex, data = CPS85) x2 &lt;- mosaic::sd(wage ~ sex, data = CPS85) x3 &lt;- mosaic::quantile(wage ~ sex, data = CPS85) x4 &lt;- mosaic::favstats(wage ~ sex, data = CPS85) x5 &lt;- gf_boxplot(wage ~ sex, data = CPS85) x6 &lt;- gf_point(wage ~ sex, data = CPS85) # Q-Q Plots qqnorm(CPS85$wage, pch = 1, frame = FALSE) qqline(CPS85$wage, col = &quot;steelblue&quot;, lwd = 2) Eine der häufigsten Ursachen für Verzerrungen in den Verteilungseigenschaften einer Variablen sind Ausreißer. Die Behandlung von Ausreißern ist ein eigenes und heftig diskutiertes Thema in der Statistik. Wir wollen uns im Rahmen dieser LV nicht weiter damit auseinandersetzen. Eine (wenngleich nicht unbedenkliche) Methode ist das sogenannte Trimmen der Daten. Nachfolgendes Beispiel zeigt eine weitere Möglichkeit3, Ausreißer aus einer Analyse zu entfernen. Es sei an dieser Stelle nochmals explizit darauf hingewiesen, dass ein beliebiges Weglassen von störenden Werten durchaus bedenklich ist und eigentlich nur im Sinne einer explorativen Analyse von Daten (was wäre, wenn die Daten keine Ausreißer hätten4?) gerechtfertigt werden kann! Kopiere den folgenden Code ins RStudio und führe diesen dann aus. Diskutiere die Ergebnisse. # panderOptions(&quot;table.split.table&quot;, 120) # pander(head(CPS85), style = &quot;rmarkdown&quot;) MW_Wage &lt;- mean(CPS85$wage, na.rm = TRUE) Med_Wage &lt;- median(CPS85$wage, na.rm = TRUE) SD_Wage &lt;- sd(CPS85$wage, na.rm = TRUE) CPS85_1 &lt;- CPS85[!CPS85$wage %in% boxplot.stats(CPS85$wage)$out,] MW_Wage_1 &lt;- mean(CPS85_1$wage, na.rm = TRUE) Med_Wage_1 &lt;- median(CPS85_1$wage, na.rm = TRUE) SD_Wage_1 &lt;- sd(CPS85_1$wage, na.rm = TRUE) # Modell0_Graph_1 ---- ggplot(CPS85, aes(x = wage)) + geom_histogram(aes(y = ..density..), binwidth = .5, colour = &quot;black&quot;, fill = &quot;white&quot;) + geom_density(alpha = .2, fill = &quot;#FF6666&quot;) + geom_vline(aes(xintercept = mean(wage, na.rm = T)), color = &quot;red&quot;, linetype = &quot;dashed&quot;, size = 1) + geom_vline(aes(xintercept=median(wage, na.rm=T)), color = &quot;blue&quot;, linetype = &quot;dotted&quot;, size = 1) + theme_bw() # Modell0_Graph_2 ggplot(CPS85_1, aes(x = wage)) + geom_histogram(aes(y = ..density..), binwidth = .5, colour = &quot;black&quot;, fill = &quot;white&quot;) + geom_density(alpha = .2, fill = &quot;#FF6666&quot;) + geom_vline(aes(xintercept = mean(wage, na.rm = T)), color = &quot;red&quot;, linetype = &quot;dashed&quot;, size = 1) + geom_vline(aes(xintercept = median(wage, na.rm = T)), color = &quot;blue&quot;, linetype = &quot;dotted&quot;, size = 1) + theme_bw() pander(shapiro.test(CPS85_1$wage), style = &quot;rmarkdown&quot;) Güteschätzung des Mittelwertsmodells Ein wichtiger Bestandteil einer Modellbildung ist die Abschätzung der Güte des jeweilig erstellten Modells. Für das Mittelwertsmodell eignet sich der Standardfehler (siehe Eq. (1)) als Kennwert zu Abschätzung der Genauigkeit des Modells. \\[\\begin{eqnarray} SE &amp;=&amp; \\frac{s}{\\sqrt{N}} \\\\ s &amp;=&amp; \\frac{\\sum_{i = 1}^{N} (x_i - \\bar{x})^2}{N-1} \\\\ \\tag{1} \\end{eqnarray}\\] Der Standardfehler (englisch: standard error, meist \\(SE\\) abgekürzt) ist die Standardabweichung der Stichprobenverteilung einer Stichprobenfunktion. In der Regel bezieht sich der Standardfehler dabei auf den Mittelwert und wird meistens dann als standard error of the mean (\\(SEM\\) abgekürzt) bezeichnet. Erläuterung zum \\(SE\\): Wenn wir viele zufällige Stichproben aus derselben Grundgesamtheit ziehen und jeweils den Mittelwert berechnen, würden diese Mittelwerte in der Regel unterschiedlich sein. Die Mittelwerte haben ihre eigene Verteilung (die wiederum ihren eigenen Mittelwert und ihre eigene Standardabweichung hat). Der Standardfehler des Mittelwerts (also der SEM und damit die Schätzung des Mittelwerts der Grundgesamtheit aus dem Mittelwert der Stichprobe) ist die Standardabweichung der Mittelwerte für alle möglichen Stichproben (mit jeder möglichen Stichprobengröße) die aus der Grundgesamtheit gezogen werden können. Offenbar spielt bei der Berechnung dieser Kennwerte die Stichprobengröße \\(N\\) eine Rolle. Welche Werte nimmt \\(s\\) und respektive \\(SE\\) ein, wenn \\(N \\rightarrow \\infty\\) geht? Wir halten fest, dass die Stichprobenstreuung \\(s\\) abhängig ist von: der Streuung \\(\\sigma\\) in der Grundgesamtheit der Stichprobengröße \\(N\\) Die Streuung in der Grundgesamtheit ist (auch wenn meist unbekannt) ein fixer Wert. Wird \\(N\\) sehr groß nähert sich die Standardabweichung diesem Wert. Im Extremfall, also wenn \\(N = N_{Pop}\\), streuen die Werte genau mit \\(\\sigma\\)! Beim Standardfehler hingegen nähert sich mit zunehmenden \\(N\\) der Wert von \\(SE\\) der Null! Im Extremfall, also wenn \\(N = N_{Pop}\\), gibt es nur mehr einen Mittelwert (und der ist gleich \\(\\mu\\)), welcher auch nicht mehr streut \\(\\Rightarrow\\) die Streuung \\(SE = 0\\). Konfidenzintervall um den Mittelwert Aus Stichproben errechnen wir einen oder mehrere verschiedene Werte, die Schätzwerte für die Grundgesamtheit darstellen sollen. Man spricht hier von Punktschätzer, da eben jeweils genau ein Wert (Anteils-, Mittelwert oder andere Größe, z. B. Regressionskoeffizient) geschätzt wird. Wünschenswerte Eigenschaften von Schätzern sind: Erwartungstreue: Der Erwartungswert (Mittelwert) der Kennwerteverteilung soll dem wahren Parameter in der Grundgesamtheit entsprechen. Effizienz: Die Streuung des Schätzers soll möglichst klein sein (d. h., die Schätzwerte sollen möglichst häufig möglichst nahe am wahren Wert liegen) Konsistenz: Mit zunehmendem Stichprobenumfang sollen Abweichungen vom wahren Wert geringer werden. Der Punktschätzer ist der beste Schätzer für den (unbekannte) Parameter der Grundgesamtheit. Dennoch ist es recht unwahrscheinlich, dass der Punktschätzer genau dem Parameter entspricht5. Daher sollte man die Punktschätzung durch eine Intervallschätzung ergänzen, die eine größere Wahrscheinlichkeit aufweist – um den Preis einer größeren Bandbreite. Die Intervallschätzung zielt nun darauf ab, einen Bereich anzugeben, der mit einer gewissen (von der Forscherin gewählten) Wahrscheinlichkeit den wahren Wert enthält (überdeckt). Dieser Bereich heißt Konfidenzintervall. Die Wahrscheinlichkeit, mit der das Intervall den wahren Wert enthält, sollte in der Regel möglichst hoch sein. Der trade-off: Je größer die gewählte Wahrscheinlichkeit, desto breiter das resultierende Intervall. Das Konfidenzintervall berechnet sich aus: \\[\\begin{equation} KI = \\bar{x} \\pm SE \\cdot t_{1 - \\frac{\\alpha}{2}; n - 1} \\tag{2} \\end{equation}\\] Die Eigenschaften des Konfidenzintervalls lassen sich sehr schön in einer Simulation von Geoff Cumming veranschaulichen. wir haben bereits bei der Mittelwertsfunktion mean() das Argument trim kennengelernt.↩ liegen z.B. Kenntnisse der Verteilungseigenschaften in der Population vor, aber die vorliegende Stichprobe ist sehr klein, könnte man eventuell↩ siehe Prof. Dr. Wolfgang Ludwig-Mayerhofer, Uni Siegen, Punkt- und Intervallschätzungen, oder Springer↩ "],
["codebook-cps85.html", "Codebook CPS85", " Codebook CPS85 Das nachfolgende Codebook stellt einen Auszug der Daten von NHANES dar. =========================================================================== wage Storage mode: double Min.: 1.000 1st Qu.: 5.250 Median: 7.780 Mean: 9.024 3rd Qu.: 11.250 Max.: 44.500 =========================================================================== educ Storage mode: integer Min.: 2.000 1st Qu.: 12.000 Median: 12.000 Mean: 13.019 3rd Qu.: 15.000 Max.: 18.000 =========================================================================== race Storage mode: integer Factor with 2 levels Values and labels N Percent 1 &#39;NW&#39; 67 12.5 2 &#39;W&#39; 467 87.5 =========================================================================== sex Storage mode: integer Factor with 2 levels Values and labels N Percent 1 &#39;F&#39; 245 45.9 2 &#39;M&#39; 289 54.1 =========================================================================== hispanic Storage mode: integer Factor with 2 levels Values and labels N Percent 1 &#39;Hisp&#39; 27 5.1 2 &#39;NH&#39; 507 94.9 =========================================================================== south Storage mode: integer Factor with 2 levels Values and labels N Percent 1 &#39;NS&#39; 378 70.8 2 &#39;S&#39; 156 29.2 =========================================================================== married Storage mode: integer Factor with 2 levels Values and labels N Percent 1 &#39;Married&#39; 350 65.5 2 &#39;Single&#39; 184 34.5 =========================================================================== exper Storage mode: integer Min.: 0.000 1st Qu.: 8.000 Median: 15.000 Mean: 17.822 3rd Qu.: 26.000 Max.: 55.000 =========================================================================== union Storage mode: integer Factor with 2 levels Values and labels N Percent 1 &#39;Not&#39; 438 82.0 2 &#39;Union&#39; 96 18.0 =========================================================================== age Storage mode: integer Min.: 18.000 1st Qu.: 28.000 Median: 35.000 Mean: 36.833 3rd Qu.: 44.000 Max.: 64.000 =========================================================================== sector Storage mode: integer Factor with 8 levels Values and labels N Percent 1 &#39;clerical&#39; 97 18.2 2 &#39;const&#39; 20 3.7 3 &#39;manag&#39; 55 10.3 4 &#39;manuf&#39; 68 12.7 5 &#39;other&#39; 68 12.7 6 &#39;prof&#39; 105 19.7 7 &#39;sales&#39; 38 7.1 8 &#39;service&#39; 83 15.5 "],
["losungen.html", "Lösungen Aufgabe_1", " Lösungen Aufgabe_1 IncomeNew &lt;- CPS85$wage ID &lt;- sample(1:534, 50) IncomeNew[ID] &lt;- IncomeNew[ID] + 180 hist(IncomeNew) # DT::datatable(data.frame(stat.desc(IncomeNew))) mean(IncomeNew, trim = .0, na.rm = TRUE) mean(IncomeNew, trim = .1, na.rm = TRUE) zurück zur Aufgabenstellung "],
["korrelationen.html", "Korrelationen Kausalität Linearität Korrelationskoeffizienten Modell", " Korrelationen Korrelation ist ein Maß für den statistischen Zusammenhang zwischen zwei Datensätzen. Unabhängige Variablen sind daher stets unkorreliert. Korrelation impliziert daher auch stochastische Abhängigkeit. Durch Korrelation wird die lineare Abhängigkeit zwischen zwei Variablen quantifiziert. Auch wenn Korrelation eine deskriptive Statistik ist, wird sie durch eine Reihe von Verfahren, wie z.B. partielle Korrelation, multiple Korrelation oder Faktorenanalyse, verfeinert. Es ist ein absolut unverzichtbares Werkzeug für viele Forschungsgebiete. Kausalität Eine relevante (statistisch signifikante) Korrelation liefert keinen Beleg für Kausalität. Vor allem in der Medizin und Psychologie suchen Forscher nach Kriterien für Kausalität. Es existieren mehrere Ansätze, nachfolgend seien hier die neun Kriterien von Hill (1965) angeführt: Stärke der Assoziation (Effektstärke): Eine geringe Effektsstärke bedeutet nicht, dass keine Beziehung zwischen den Variablen existiert, aber bei einer größeren Effektstärke ist eine kausale Beziehung wahrscheinlicher. Konsistenz (Reproduzierbarkeit): Übereinstimmende Befunde, festgestellt durch verschiedene Personen an verschiedenen Orten mit verschiedenen Stichproben, verstärkt die Wahrscheinlichkeit eines Effekts. Spezifität: Kausalität ist wahrscheinlich, wenn es keine andere bessere Erklärung für das Auftreten des Ereignisses gibt. Je spezifischer die Beziehung zwischen einem Faktor und einem Effekt ist, desto höher die Wahrscheinlichkeit eines kausalen Zusammenhangs. Zeitbedingtheit: Die Ursache muss der Wirkung vorausgehen (wenn erwartet wird, dass es eine Verzögerung nach der Ursache und dem erwarteten Effekt gibt, dann muss der Effekt nach der Verzögerung eintreten). Dosis-Wirkungs-Beziehung: Größere Exposition sollte gewöhnlich auch zu einer größeren Auftretenshäufigkeit des Effekts führen. In einigen Fällen jedoch kann bereits die bloße Anwesenheit eines Faktors einen Effekt auslösen. In anderen Fällen wird der umgekehrte Fall beobachtet: die Anwesenheit eines Faktors reduziert die Auftretenshäufigkeit. (Biologische) Plausibilität: Ein plausibler Mechanismus zwischen Ursache und Wirkung ist hilfreich (allerdings fügte Hill auch hinzu, dass das Verständnis von Ursache und Wirkung durch unseren aktuellen Wissensstand begrenzt sein kann). Koheränz: Die Stimmigkeit zwischen epidemiologischen- und Laborbefunden erhöht die Wahrscheinlichkeit eines Effekts. Hill fügte allerdings hinzu, dass das Fehlen eines Laborbefundes nicht einen epidemiologischen Effekt auf die Beziehung ungültig machen kann. Experiment: Es ist wahrscheinlicher, dass eine Beziehung kausal ist, wenn sie experimentell verifiziert werden kann. Analogie: Der Effekt ähnlicher Faktoren darf berücksichtigt werden. Linearität Ein Korrelationskoeffizient zeigt die Stärke eines linearen Zusammenhangs zwischen zwei Variablen. Aber der Wert von \\(r\\) charakterisiert nicht die genaue Art des Zusammenhangs oder das Aussehen des Punktdiagramms beider Variablen6. Abbildung 2: Korrelation und Linearität Korrelationskoeffizienten Neben dem Pearson-Produkt-Moment-Korrelationskoeffizienten \\(r\\) existieren noch etliche weitere Korrelationskoeffizienten und Zusammenhangsmaße. Die meisten hiervon sind Sonderfälle der Pearson-Produkt-Moment-Korrelation. Die Tabelle (unten) zeigt, wann welcher Koeffizient berechnet werden soll, abhängig von dem Skalenniveau der beteiligten Variablen. Abbildung 3: verschiedene Korrelationskoeffizienten Weiter Infos zu den einzelnen Korrelationskoeffizienten sind der Literatur zu entnehmen. Eine übersichtliche Darstellung findet man auch unter folgendem Link. Modell Bereits bei der deskriptiven Statistik haben wir mit dem Maß der Varianz (\\(s^2\\)) einen Kennwert definiert, der die Schwankungen bezüglich des entsprechenden Mittelwertes beschreibt. Per Definition ist die Varianz die durchschnittliche Summe der quadrierten Abweichungen zum Mittelwert, also: \\[\\begin{equation} s^2 = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x})^2}{N-1} \\tag{3} \\end{equation}\\] Betrachtet man zwei (normalverteilte) intervallskalierte Variablen \\(x\\) und \\(y\\), dann lässt sich diese Idee auch als ein Kennwert der gemeinsamen Variablität der beiden Variablen definieren: \\[\\begin{equation} cov(x,y) = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x}) \\cdot (y_i - \\bar{y})}{N-1} \\tag{4} \\end{equation}\\] Dieser Kennwert nennt sich Kovarianz (\\(cov\\)). Da dieser Kennwert an die entsprechenden Einheiten der Variablen gebunden ist, normiert man i.A. dieses Maß durch das Produkt der jeweiligen Standardabweichung \\(s_x\\) und \\(s_y\\). Dieses normierte Maß bezeichnet man als Korrelationskoeffizient (\\(r\\)): \\[\\begin{equation} r(x,y) = \\frac{cov(x,y)}{s_x \\cdot s_y} \\tag{5} \\end{equation}\\] Beispiel Anhand des bereits verwendeten Datensatzes (CPS85) wollen wir die Beziehung der Variablen Gehalt (wage), Ausbildung (educ) und Berufserfahrung (exper) berechnen und graphisch darstellen. Kopiere den folgenden Code ins RStudio und führe diesen dann aus. Diskutiere die Ergebnisse. M &lt;- data.frame(wage = CPS85$wage, educ = CPS85$educ, exper = CPS85$exper) Korr_1 &lt;- cor(M) pander(Korr_1, style = &quot;rmarkdown&quot;) # DT::datatable(round(Korr_1,2)) corrplot(cor(M), method = &quot;ellipse&quot;) Abbildungen aus Matheguru↩ "],
["einfache-regression.html", "Einfache Regression Definition Modellanwendung Residualanalyse", " Einfache Regression Die Regression basiert auf der Korrelation und ermöglicht die bestmögliche Vorhersage für eine Variable. Im Gegensatz zur Korrelation muss hierbei festgelegt werden, welche Variable durch eine andere Variable vorhergesagt werden soll. Die Variable die vorhergesagt werden soll nennt man bei der Regression Kriterium (\\(y\\)). Die Variable die für die Vorhersage eingesetzt wird bezeichnet man als Prädiktor (\\(x_i\\))7. Anhand des Prädiktors wird demzufolge das Kriterium vorhergesagt. Definition Die formale Definition eines einfachen linearen Modells ist: \\[\\begin{equation} y = b_0 + b_1 \\cdot x_1 + \\varepsilon_1 \\tag{6} \\end{equation}\\] Die wesentlichen Parameter dieses einfachen Modells sind: Intercept \\(b_0\\): jener Wert den \\(y\\) einnimmt, wenn \\(x_1 = 0\\) ist. Steigung \\(b_1\\): die Zunahme von \\(y\\), wenn \\(x_1\\) sich um eine Einheit erhöht. Des Weiteren berücksichtigt dieses Modell auch einen Fehler (\\(\\varepsilon_1\\)). Damit kommt auch ein ganz zentraler Teil bei der Modellbildung zum Ausdruck. Die meisten Modelle definieren sich also aus: \\[\\begin{equation} \\textrm{wahrer Wert} = \\textrm{Modell} + \\textrm{Fehler} \\tag{7} \\end{equation}\\] Daraus lässt sich auch folgende Erkenntnis bezüglich des Modells direkt ableiten: Je kleiner die Summe der Fehler sind, desto besser ist das Modell. Je genauer das Modell, desto kleiner wird auch der Fehler sein. Mit dieser Erkenntnis wird auch klar, dass i.A. ein ganz einfaches Modell (mit einem einzigen Prädiktor) nur zu einer bedingten Reduktion des Fehlers geeignet ist. Wir werden uns im weiteren Verlauf mit erweiterten Modellen beschäftigen, wollen aber zunächst die Eigenschaften des einfachen linearen Modells näher betrachten. Im folgenden Link findet man eine gute Veranschaulichung des einfachen linearen Modells. Betrachtet man das Modell isoliert (also ohne Fehlerterm), ist folgende Schreibweise üblich: \\[\\begin{equation} \\hat{y} = b_0 + b_1 \\cdot x_1 \\tag{8} \\end{equation}\\] Berechnung der Koeffizienten Für die Berechnung der Koeffizienten wird das Kriterium der kleinste Quadrate (MLS) angewandet. Einfach ausgedrückt wird eine Gerade durch die beobachteten Daten gesucht, die folgenden Eigenschaften aufweist: die Summe der quadratischen Abstände jeder Beobachtung zum entsprechenden Punkt auf der Geraden ist ein Minimum, also \\(\\sum_{i=1}^{N} \\varepsilon_1^2 = min\\). es gibt keine andere Gerade die eine kleinere Summe dieser Fehler liefert. Die Berechnung der Parameter entspricht daher einer Extremwertaufgabe, d.h. die partiellen Ableitungen werden auf Null gesetzt. Daraus lassen sich dann die Parameter \\(b_0, b_1\\) berechnent. Details dazu siehe Wikipedia. Modellanwendung Zur Anwendung eines einfachen linearen Modell betrachten wir wiederum die bereits bekannten Daten aus dem Datensatz CPS85. Diese Mal wollen wir das Gehalt (wage) durch die Ausbildungsdauer (educ in Jahren) vorhersagen. Formal lautet das Modell demnach: \\[\\begin{equation} \\hat{\\textrm{wage}} = b_0 + b_1 \\cdot \\textrm{educ} \\tag{9} \\end{equation}\\] Die Werte der Parameter \\(b_0, b_1\\) können für dieses Beispiel entsprechend der obigen Erläuterung folgendermaßen interpretiert werden: Für eine Person mit keiner Ausbildung (\\(\\textrm{wage} = x_1 = 0\\)) wird durch das Modell ein Einkommen \\(y = b_0\\) vorhergesagt. Erhöht man die Ausbildungsdauer \\(x_1\\) um ein Jahr, steigt der Gehalt \\(y\\) um das \\(b_1\\)-fache an. Kopiere zur Veranschaulichung folgenden Code in dein R-Script und führe diesen aus. DF &lt;- CPS85 ggplot(CPS85, aes(x = educ, y = wage)) + geom_point() + geom_smooth(method=lm, se=FALSE) + theme_bw() model_1 &lt;- lm(wage ~ educ, data = CPS85) pander(summary(model_1)) Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.746 1.045 -0.7135 0.4758 educ 0.7505 0.07873 9.532 5.474e-20 Fitting linear model: wage ~ educ Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 534 4.754 0.1459 0.1443 Die in der Tabelle angegebenen Werte der Spalte Estimate entsprechen dabei den Parametern \\(b_0, b_1\\) des Modells. Eine weitere wesentliche Kennzahl für die Interpretation des Modells ist der Spalte \\(R^2\\) zu entnehmen. Dieser Wert wird als Determinationskoeffizient8 bezeichnet. Umgerechnet in % (im vorliegenden Beispiel also 14.43%) besagt der Wert, wie viel der Variablität des Gehaltes durch den Prädiktor Ausbilung erklärt wird. Wir werden im weiteren Verlauf noch öfter auf diesen Kennwert zurückkommen. Welche Gehälter würden für Ausbildungszeiten zwischen 10 und 14 Jahren vorhergesagt werden? Kopiere folgenden Code in dein R-Script und führe diesen aus. Änder auch den Wertebereich der Prädiktoren und beobachte was dabei passiert! new_input &lt;- data.frame(educ = 10:14) pander(predict(model_1, newdata = new_input), style = &quot;rmarkdown&quot;) 1 2 3 4 5 6.759 7.509 8.26 9.01 9.76 Im Vergleich zum Mittelwert-Modell zeigt sich mit steigender Ausbildung ein höheres Einkommen. Der Fehler bei der Vorhersage des Einkommens wird sich daher durch diese Modellvorstellung verringern (mehr zur Abschätzung der Fehlerreduktion später). Residualanalyse Ein zentrales Thema der Modellbildung ist die Beurteilung und (statistische) Auswertung der Abweichungen des Modells von den Beobachtungen (Fehler, Residum). Folgende Kennwerte bilden die Möglichkeit, die Güte des Modells abzuschätzen: Vorhergesagte Werte: vorhergesagte Werte der Regressionsgleichung (= Werte die auf der Geraden liegen). Residuen: tatsächliche Wert der abhängigen Variablen minus des vorhergesagten Werts aus der Regressionsgleichung. Distanzen: Maße zum Auffinden von Fällen mit ungewöhnlichen Wertekombinationen bei den unabhängigen Variablen und von Fällen, die einen großen Einfluss auf das Modell haben könnten. Vorhersageintervalle: obere und untere Grenzen sowohl für Mittelwert als auch für einzelne Vorhersageintervalle. Einflussstatistiken: Änderung in den Regressionskoeffizienten (DfBeta(s)) und vorhergesagten Werten (DfFit), die sich aus dem Ausschluss eines bestimmten Falls ergeben. Die Detailanalyse der Residuen soll dazu dienen, die Güte eines Modells besser beurteilen zu können. CPS85_Res &lt;- data.frame(Res = round(resid(model_1), 2), StdRes = round(rstandard(model_1), 2), StudRes = round(rstudent(model_1), 2), # Cook = round(cooks.distance(model_1), 2), # DFBeta = round(dfbeta(model_1), 2), DF5Fit = round(dffits(model_1), 2), # Lev = round(hatvalues(model_1), 2), CovRat = round(covratio(model_1), 2)) pander(head(CPS85_Res)) Res StdRes StudRes DF5Fit CovRat 2.24 0.47 0.47 0.03 1.01 -2.76 -0.58 -0.58 -0.03 1 -4.46 -0.94 -0.94 -0.04 1 2.24 0.47 0.47 0.02 1.01 6.74 1.42 1.42 0.07 1 -2.26 -0.48 -0.48 -0.03 1.01 Mit der Residualanalyse kann man auf relativ einfache Weise jene Werte ermitteln (und auch graphisch darstellen), die z.B. um mehr als eine Standardabweichung abweichen. Diese Werte könnte man nochmals genauer untersuchen und gegebenenfalls vor einer weiterführenden Analyse ausschließen9. Keinesfalls sollte sie jedoch dazu verwendet werden, um einen erwünschten Effekt durch schrittweises löschen störender Daten zu erreichen! Kopiere folgenden Code in dein R-Script und führe diesen aus. # Liste standardisierte Residuen &gt; |1| Ind_Res &lt;- which((CPS85_Res$StdRes &gt; 1 | CPS85_Res$StdRes &lt; -1) == TRUE) # Anzeige der Werte von wage und educ sowie der Standardisierten Residuen # fÃ¼r jene FÃ¤lle, deren Residuen Ã¼ber 1 SD abweichen. # pander(data.frame(Indizes = Ind_Res, # wage = CPS85$wage[Ind_Res], # educ = CPS85$educ[Ind_Res] , # CPS85_Res$StdRes[Ind_Res])) p_Res1 &lt;- ggplot(CPS85, aes(x = educ, y = wage)) + geom_point() + geom_point(data=CPS85[Ind_Res,],colour=&quot;red&quot;,size=3) + theme_bw() print(p_Res1, comment = FALSE) wobei \\(i\\) als Index für die Anzahl der verwendeten Prädiktoren steht. Bei einem einfachen Modell ist \\(i = 1\\)↩ häufig auch als Varianzaufklärung↩ der Ausschluss von Werten ist nur dann erlaubt, wenn eine entsprechende Begründung (nachvollziehbarer Messfehler, falsche Datenübertragung, etc.) vorliegt!↩ "],
["references.html", "References", " References "]
]
